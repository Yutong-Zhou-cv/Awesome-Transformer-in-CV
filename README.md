# <p align=center>`awesome Transformer in CV papers`</p>

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A collection of resources on Transformer in CV.

## <span id="head-content"> *Content* </span>
* - [ ] [1. Description](#head1)
* - [ ] [2. Survey](#head2)
* - [ ] [3. Paper With Code](#head2)
  * - [ ] [Image Classification](#head-Image-Classification)
  * - [ ] [Object Detection](#head-Object-Detection)
  * - [ ] [Instance Segmentation](#head-Instance-Segmentation)
  * - [ ] [Semantic Segmentation](#head-Semantic-Segmentation)
  * - [ ] [Video Understanding](#head-Video-Understanding)
  * - [ ] [Dense Prediction](#head-Dense-Prediction)
  * - [ ] [GAN](#head-GAN)
* [*Contact Me*](#head3)

 ## <span id="head1"> *1. Description* </span>
 
## <span id="head2"> *2. Survey* </span>

## <span id="head3"> *3. Paper With Code* </span>
  * <span id="head-Image-Classification"> **Image Classification**  </span>
      * (arXiv preprint 2021) **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**, Ze Liu et al. [[Paper](https://arxiv.org/pdf/2103.14030.pdf)] [[Code](https://github.com/microsoft/Swin-Transformer)]
  * <span id="head-Object-Detection"> **Object Detection**  </span>
      * (arXiv preprint 2021) **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**, Ze Liu et al. [[Paper](https://arxiv.org/pdf/2103.14030.pdf)] [[Code](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection)]‚≠êSOTA on [COCO test-dev, COCO minival](https://cocodataset.org/#home)
  * <span id="head-Instance-Segmentation"> **Instance Segmentation**  </span>
      * (arXiv preprint 2021) **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**, Ze Liu et al. [[Paper](https://arxiv.org/pdf/2103.14030.pdf)] [[Code](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection)]‚≠êSOTA on [COCO test-dev, COCO minival](https://cocodataset.org/#home)
  * <span id="head-Semantic-Segmentation"> **Semantic Segmentation**  </span>
      * (arXiv preprint 2021) **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**, Ze Liu et al. [[Paper](https://arxiv.org/pdf/2103.14030.pdf)] [[Code](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation)]‚≠êSOTA on [ADE20K dataset, ADE20K val](https://groups.csail.mit.edu/vision/datasets/ADE20K/)
  * <span id="head-Video-Understanding"> **Video Understanding**  </span>
      * (arXiv preprint 2021) **Is Space-Time Attention All You Need for Video Understanding?**, Gedas Bertasius et al. [[Paper](https://arxiv.org/pdf/2102.05095.pdf)] [[Code](https://github.com/facebookresearch/TimeSformer)]üå±Convolution-free
 * <span id="head-Dense-Prediction"> **Dense Prediction**  </span>
      * (arXiv preprint 2021) **Vision Transformers for Dense Prediction**, Ren√© Ranftl et al. [[Paper](https://arxiv.org/pdf/2103.13413.pdf)] [[Code](https://github.com/intel-isl/DPT)]‚≠êSOTA on [Monocular Depth Estimation on NYU-Depth V2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)
 * <span id="head-GAN"> **GAN**  </span>
      * (arXiv preprint 2021) **TransGAN: Two Transformers Can Make One Strong GAN**, Yifan Jiang et al. [[Paper](https://arxiv.org/pdf/2102.07074.pdf)] [[Code](https://github.com/VITA-Group/TransGAN)]üå±Convolution-free


## <span id="head3"> *Contact Me* </span>

* [Yutong ZHOU](https://github.com/Yutong-Zhou-cv) in [Interaction Laboratory, Ritsumeikan University.](https://github.com/Rits-Interaction-Laboratory) („Å•Ôø£0Ôø£)„Å•

* If you have any question, please feel free to contact Yutong ZHOU (E-mail: <zhou@i.ci.ritsumei.ac.jp>).
